<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="VisionDirector: Vision-Language Guided Closed-Loop Refinement for Generative Image Synthesis.">
  <meta name="keywords"
        content="Vision-Language Models, Image Generation, Image Editing, Diffusion Models, Agents, Reinforcement Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>
    VisionDirector: Vision-Language Guided Closed-Loop Refinement
    for Generative Image Synthesis
  </title>

  <!-- Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <!-- Bulma CSS -->
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

  <!-- Academic Page CSS (same style as GeoText / Nerfies) -->
  <link rel="stylesheet" href="./static/css/index.css">

  <script defer src="./static/js/fontawesome.all.min.js"></script>
</head>

<body>

<!-- ================= TITLE / AUTHORS ================= -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">

          <h1 class="title is-1 publication-title">
            VisionDirector: Vision-Language Guided Closed-Loop Refinement
            for Generative Image Synthesis
          </h1>

          <!-- Authors -->
          <div class="is-size-5 publication-authors">
            <span class="author-block">Meng Chu<sup>1</sup>,</span>
            <span class="author-block">Senqiao Yang<sup>2</sup>,</span>
           <span class="author-block">Haoxuan Che<sup>3</sup><sup>*†</sup>,</span>
            <span class="author-block">Suiyun Zhang<sup>3</sup>,</span>
            <span class="author-block">Xichen Zhang<sup>1</sup>,</span>
            <span class="author-block">Shaozuo Yu<sup>2</sup>,</span>
            <span class="author-block">Haokun Gui<sup>1</sup>,</span>
            <span class="author-block">Zhefan Rao<sup>1</sup>,</span>
            <span class="author-block">Dandan Tu<sup>3</sup>,</span>
            <span class="author-block">Rui Liu<sup>3</sup><sup>*</sup>,</span>
            <span class="author-block">Jiaya Jia<sup>1</sup></span>
          </div>

          <!-- Affiliations -->
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>HKUST</span>,
            <span class="author-block"><sup>2</sup>CUHK</span>,
            <span class="author-block"><sup>3</sup>Huawei Research</span>
          </div>

          <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>*</sup>Corresponding authors</span>
          </div>

          <!-- Links -->
          <div class="publication-links">

            <span class="link-block">
              <a href="https://arxiv.org/abs/XXXX.XXXXX"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="ai ai-arxiv"></i>
                </span>
                <span>Paper</span>
              </a>
            </span>

            <span class="link-block">
              <a href="./static/pdfs/supplementary.pdf"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fas fa-file-pdf"></i>
                </span>
                <span>Supplementary</span>
              </a>
            </span>

            <span class="link-block">
              <a href="https://github.com/your-org/VisionDirector"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
              </a>
            </span>

            <span class="link-block">
              <a href="https://github.com/your-org/LGBench"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fas fa-database"></i>
                </span>
                <span>LGBench Dataset</span>
              </a>
            </span>

          </div>

        </div>
      </div>
    </div>
  </div>
</section>

<!-- ================= ABSTRACT ================= -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          Professional image creation often relies on long, multi-goal instructions
          specifying global composition, local object placement, typography, and
          stylistic constraints. While modern diffusion models achieve strong visual
          fidelity, they frequently fail to satisfy such tightly coupled objectives.
          <br><br>
          To systematically expose this gap, we introduce <b>LongGoalBench (LGBench)</b>,
          a dual-modality benchmark comprising 2,000 text-to-image and image-to-image
          tasks with over 29,000 annotated goals and automated goal-level verification.
          <br><br>
          We further propose <b>VisionDirector</b>, a vision-language guided closed-loop
          framework that decomposes long instructions into structured goals, dynamically
          plans generation or editing actions, and verifies goal satisfaction after each
          step without retraining diffusion backbones.
        </div>
      </div>
    </div>
  </div>
</section>

<!-- ================= FIGURES ================= -->
<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Framework Overview</h2>
        <div class="publication-image">
          <img src="./static/images/fig1_overview.png" alt="Framework Overview">
        </div>
        <div class="content has-text-justified">
          VisionDirector introduces a director-style vision–language agent that
          decomposes long, multi-goal instructions into structured goals and
          supervises generation and editing through a closed-loop process with
          verification and rollback.
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">LGBench: Long-Goal Benchmark</h2>
        <div class="publication-image">
          <img src="./static/images/fig2_lgbench.png" alt="LGBench Dataset">
        </div>
        <div class="content has-text-justified">
          LGBench evaluates long-horizon instruction following beyond single-shot
          generation, covering both text-to-image and image-to-image tasks with
          diverse goal types.
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Adaptive Planning Behavior</h2>
        <div class="publication-image">
          <img src="./static/images/fig3_planning_behavior.png" alt="Planning Behavior">
        </div>
        <div class="content has-text-justified">
          VisionDirector adaptively switches between one-shot generation and staged
          refinement as instruction complexity increases, demonstrating rational
          planning behavior.
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Qualitative Results</h2>
        <div class="publication-image">
          <img src="./static/images/fig4_results.png" alt="Qualitative Results">
        </div>
        <div class="content has-text-justified">
          VisionDirector consistently improves multi-goal adherence on complex
          generation and editing tasks, particularly for layout, typography,
          and identity preservation.
        </div>
      </div>
    </div>

  </div>
</section>

<!-- ================= BIBTEX ================= -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
<pre><code>@article{chu2026visiondirector,
  title   = {VisionDirector: Vision-Language Guided Closed-Loop Refinement for Generative Image Synthesis},
  author  = {Chu, Meng and Yang, Senqiao and Che, Haoxuan and others},
  journal = {CVPR},
  year    = {2026}
}</code></pre>
  </div>
</section>

<!-- ================= FOOTER ================= -->
<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <p>
        This website is licensed under a
        <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">
          Creative Commons Attribution-ShareAlike 4.0 International License
        </a>.
      </p>
      <p>
        Website template adapted from
        <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
      </p>
    </div>
  </div>
</footer>

</body>
</html>
